{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "405e39fc",
   "metadata": {},
   "source": [
    "## Every computation happens in pytorch through tensors only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d037c9",
   "metadata": {},
   "source": [
    "\n",
    "#### Theory of the topic.\n",
    "\n",
    "#### If we have defined a tensor \n",
    "\n",
    "#### x = torch.tensor([3.])\n",
    "#### y = torch.tensor([4.])\n",
    "\n",
    "#### and our objective function is following\n",
    "\n",
    "#### z = x**2 + 4*y\n",
    "\n",
    "#### now if we want to differentiate z w.r.t x, y then we will write \n",
    "\n",
    "#### x = torch.tensor([3.]).requires_grad_(True)\n",
    "#### y = torch.tensor([4.]).requires_grad_(True)\n",
    "\n",
    "#### z.backward()  ----> this is how we do the back propagation.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0f030",
   "metadata": {},
   "source": [
    "### Autograd in pytorch.\n",
    "\n",
    "#### grad_fn is the derivative of the operation, it will be associated with those are not leaf nodes.\n",
    "\n",
    "#### w.grad_fn  represents the gradient of w and torch will look what was used to make w.\n",
    "\n",
    "#### And is there was nothing used to create that function like x as input then x.grad_fn will be NONE. Hence x is here a leaf node.\n",
    "\n",
    "#### grad is for leaf nodes,  those were not created by any other node. Because we are differentiating w.r.t to leafs and updating them like we do for weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65984d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import pandas as pd \n",
    "import torch.nn as nn\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "206e101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2.0]).requires_grad_(True)\n",
    "y = torch.tensor([3.0]).requires_grad_(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4675fdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.log(x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fab14d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # now we can find which one is leaf node or not we can check by running the below code and we can see that w is \n",
    "# not a leaf node.\n",
    "    \n",
    "x.is_leaf, y.is_leaf, w.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7773df33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<LogBackward0 at 0x169096610>, None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad_fn, x.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe9ee6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ry/cpzzpnh51yvfh6ctzlsm0jsw0000gn/T/ipykernel_8279/1622017159.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  w.grad, x.grad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in the below code we have not initiated the backward function hence we are getting both the grads = None\n",
    "w.grad, x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99000253",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.backward()\n",
    "\n",
    "# here x.grad gives the differentiation of w w.r.t x and then put the value of x = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fd6ceaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ry/cpzzpnh51yvfh6ctzlsm0jsw0000gn/T/ipykernel_8279/3850471554.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  w.grad, x.grad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, tensor([0.5000]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad, x.grad"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1a0dbea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2949236a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dcdafd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aeef00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfd0b20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
